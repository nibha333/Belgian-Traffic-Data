{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/45230448/how-to-get-reproducible-result-when-running-keras-with-tensorflow-backend\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.data\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
    "from keras.initializers import glorot_uniform, he_normal\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, LeaveOneOut, KFold, cross_val_score,cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import skimage.data\n",
    "import skimage.transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.python.framework import ops\n",
    "import warnings\n",
    "\n",
    "\n",
    "#code tutorial\n",
    "#https:////github.com//PacktPublishing//Practical-Convolutional-Neural-Networks//blob//master//Chapter03//Traffic//custom.py\n",
    "\n",
    "Train_IMAGE_DIR = \"C://New folder//OneDrive//ai class//Final Project//BelgiumTSC_Training//\"\n",
    "Test_IMAGE_DIR = \"C://New folder//OneDrive//ai class//Final Project//BelgiumTSC_Testing//\"\n",
    "\n",
    "# Train_IMAGE_DIR = 'C:/Users/bipul/niva/BelgiumTSC_Training/'\n",
    "# Test_IMAGE_DIR = 'C:/Users/bipul/niva/BelgiumTSC_Testing/'\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all the subdirectories of the data folder (i.e. traing or test). Each folder represents an unique label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    \n",
    "    # Iterate for loop through the label directories and collect the data in two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "        file_names = [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
    "\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "# Load training and testing datasets.\n",
    "# train_data_dir = os.path.join(Train_IMAGE_DIR, \"Training\")\n",
    "# test_data_dir = os.path.join(Test_IMAGE_DIR, \"Testing\")\n",
    "\n",
    "\n",
    "\n",
    "images1, labels1 = load_data(Train_IMAGE_DIR)\n",
    "images2, labels2 = load_data(Test_IMAGE_DIR)\n",
    "\n",
    "# images1, labels1 = load_data(train_data_dir)\n",
    "# images2, labels2 = load_data(test_data_dir)\n",
    "\n",
    "#combine training and testing data\n",
    "\n",
    "# images = images1 + images2\n",
    "# labels = labels1 + labels2\n",
    "\n",
    "# #combine training and testing data\n",
    "\n",
    "images = images1 + images2\n",
    "labels = labels1 + labels2\n",
    "\n",
    "\"Resize images\"\n",
    "images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
    "                for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convert labels to categorical'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"Resize images\"\n",
    "images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n",
    "                for image in images]\n",
    "\n",
    "# \"convert from list to array\"\n",
    "labels_array = np.array(labels)\n",
    "# images_array = np.array(images32)\n",
    "\n",
    "\"scale images\"\n",
    "images_array = np.array(images32)\n",
    "images_array = images_array.astype('float32') / 255.\n",
    "\n",
    "# \"Reshape the image data into rows\"\n",
    "# # images_array = np.reshape(images_array, (images_array.shape[0], -1))\n",
    "\n",
    "\"convert labels to categorical\"\n",
    "# labels_cat = np_utils.to_categorical(labels_array, 62)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"split data into training and testing - 2-tier scheme\"\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(images_array,labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5321, 32, 32, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(num_neuron_l1,num_neuron_l2,activ_fn,wt_initial,drop_out):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_neuron_l1,input_shape=(3072,), kernel_initializer=wt_initial))\n",
    "    \n",
    "    model.add(Activation(activ_fn))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(num_neuron_l2, kernel_initializer=wt_initial))\n",
    "    model.add(Activation(activ_fn))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(62,kernel_initializer=wt_initial))\n",
    "    model.add(Activation('softmax'))    \n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy']) \n",
    "              \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5321, 3072)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Reshape the image data into rows\"\n",
    "X_train_reshape = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test_reshape = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_train_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"create a data generator\"\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True,\n",
    "                             horizontal_flip = True,\n",
    "                         width_shift_range = 0.1,\n",
    "                         height_shift_range = 0.1,\n",
    "                         zoom_range = 0.1,\n",
    "                         rotation_range = 10\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    \n",
    "    img_aug=next(datagen.flow(img, batch_size=1))\n",
    "    \n",
    "    return(img_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial\n",
    "#https://towardsdatascience.com/my-tryst-with-deep-learning-german-traffic-data-set-with-keras-87970dfb18b7\n",
    "\"create equal sample size for each class\"\n",
    "\n",
    "\n",
    "labels_array= np.asarray(labels)\n",
    "\n",
    "X_train_aug = []\n",
    "y_train_aug = []\n",
    "X_aug_1 = []\n",
    "Y_aug_1 = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,62):\n",
    "    \n",
    "    class_records=np.where(labels_array==i)[0].size\n",
    "    max_records=200\n",
    "    samples=images_array[np.where(labels_array==i)[0]]  \n",
    "    X_aug=[]\n",
    "    Y_aug=[i]*max_records\n",
    "    for x in range(max_records):\n",
    "        img=samples[x % class_records]\n",
    "        img=img.reshape((1,) + img.shape)\n",
    "        trans_img=data_augment(img)\n",
    "        trans_img=trans_img.reshape(32,32,3)\n",
    "        X_aug.append(trans_img)\n",
    "    X_train_aug = X_train_aug + X_aug\n",
    "    y_train_aug = y_train_aug + Y_aug\n",
    "\n",
    "    Y_aug_1 = Y_aug_1 + Y_aug\n",
    "    X_aug_1 = X_aug_1 + X_aug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"convert from list to array\"\n",
    "\n",
    "new_label= np.asarray(Y_aug_1)\n",
    "new_data= np.asarray(X_aug_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datas=np.reshape(new_data, ( new_data.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"divide data into 4 folds\"\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "fold1, fold2, fold3, fold4 = kf.split(new_datas)\n",
    "\n",
    "xtrain1,xtest1=new_datas[fold1[0]],new_datas[fold1[1]]\n",
    "ytrain1,ytest1=new_label[fold1[0]],new_label[fold1[1]]\n",
    "\n",
    "xtrain2,xtest2=new_datas[fold2[0]],new_datas[fold2[1]]\n",
    "ytrain2,ytest2=new_label[fold2[0]],new_label[fold2[1]]\n",
    "\n",
    "xtrain3,xtest3=new_datas[fold3[0]],new_datas[fold3[1]]\n",
    "ytrain3,ytest3=new_label[fold3[0]],new_label[fold3[1]]\n",
    "\n",
    "xtrain4,xtest4=new_datas[fold4[0]],new_datas[fold4[1]]\n",
    "ytrain4,ytest4=new_label[fold4[0]],new_label[fold4[1]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nibha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\nibha\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_ann=create_model(500,500,'elu','glorot_uniform',0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nibha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279c4ee8748>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ann.fit(x=xtrain1,y= ytrain1, validation_data=(xtest1, ytest1),epochs=10, batch_size=128, shuffle=False, verbose=0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model_ann.predict(xtest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred = np.argmax(np.round(pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 15, 15, 15])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.06451612903225806, 0.9354838709677419, None)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import sensitivity_specificity_support\n",
    "report2= sensitivity_specificity_support(ytest1,ann_pred,average='weighted')\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      1.00      0.12       200\n",
      "           1       0.00      0.00      0.00       200\n",
      "           2       0.00      0.00      0.00       200\n",
      "           3       0.00      0.00      0.00       200\n",
      "           4       0.00      0.00      0.00       200\n",
      "           5       0.00      0.00      0.00       200\n",
      "           6       0.00      0.00      0.00       200\n",
      "           7       0.00      0.00      0.00       200\n",
      "           8       0.00      0.00      0.00       200\n",
      "           9       0.00      0.00      0.00       200\n",
      "          10       0.00      0.00      0.00       200\n",
      "          11       0.00      0.00      0.00       200\n",
      "          12       0.00      0.00      0.00       200\n",
      "          13       0.00      0.00      0.00       200\n",
      "          14       0.00      0.00      0.00       200\n",
      "          15       0.00      0.00      0.00       100\n",
      "\n",
      "   micro avg       0.06      0.06      0.06      3100\n",
      "   macro avg       0.00      0.06      0.01      3100\n",
      "weighted avg       0.00      0.06      0.01      3100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report1= classification_report(ytest1,ann_pred)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06451612903225806\n"
     ]
    }
   ],
   "source": [
    "report3=accuracy_score(ytest1, ann_pred)\n",
    "print(report3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "#     scaler= StandardScaler(with_std=False)\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "  \n",
    "    cnn2.fit(x=X_train,y= y_train, validation_data=(X_test, y_test),epochs=30, batch_size=128, shuffle=False, verbose=0)\n",
    "    preddd= cnn2.predict(X_test)\n",
    "    cnn_pred = np.argmax(np.round(preddd), axis=1)\n",
    "    \n",
    "    \n",
    "    report1= classification_report(y_test,cnn_pred)\n",
    "    print(report1)\n",
    "    report2= sensitivity_specificity_support(y_test,cnn_pred,average='weighted')\n",
    "    print(report2)\n",
    "    report3=accuracy_score(X_test, y_test)\n",
    "    preint(report3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2 = get_eval(xtrain2,xtest2,ytrain2,ytest2)\n",
    "score3 = get_eval(xtrain3,xtest3,ytrain3,ytest3)\n",
    "score4 = get_eval(xtrain4,xtest4,ytrain4,ytest4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
